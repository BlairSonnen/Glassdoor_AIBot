{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzCJ8Jhu4d-Y"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q45PrKcU3lKE"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93BK1XGpz0pk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimized RAG Ingestion Script (Batch Embedding + Chunk Truncation)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_if_missing(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "required_packages = [\n",
        "    \"sentence_transformers\",\n",
        "    \"chromadb\",\n",
        "    \"pandas\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for pkg in required_packages:\n",
        "    install_if_missing(pkg)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from chromadb import PersistentClient\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "CHROMA_PATH = \"/content/chroma_local_tmp\"\n",
        "Path(CHROMA_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "client = PersistentClient(path=CHROMA_PATH)\n",
        "collection = client.get_or_create_collection(\"csv_documents\")\n",
        "\n",
        "def ingest_csv_optimized(filepath, chunk_size=1000, truncate_len=3000):\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, low_memory=False)\n",
        "        total_rows = len(df)\n",
        "        print(f\"\\nüìÑ Processing: {filepath.name} ({total_rows} rows)\")\n",
        "\n",
        "        texts, metadatas, ids = [], [], []\n",
        "        for start in tqdm(range(0, total_rows, chunk_size), desc=f\"Chunks in {filepath.name}\"):\n",
        "            chunk = df.iloc[start:start + chunk_size]\n",
        "            chunk_text = chunk.to_string(index=False)\n",
        "            if len(chunk_text) > truncate_len:\n",
        "                chunk_text = chunk_text[:truncate_len]\n",
        "\n",
        "            texts.append(chunk_text)\n",
        "            metadatas.append({\"filename\": str(filepath), \"start_row\": int(start)})\n",
        "            ids.append(f\"{filepath.stem}_{start}\")\n",
        "\n",
        "        # batch embedding\n",
        "        embeddings = embed_model.encode(texts, batch_size=8)\n",
        "\n",
        "        collection.add(\n",
        "            documents=texts,\n",
        "            embeddings=[e.tolist() for e in embeddings],\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "        gc.collect()\n",
        "        print(f\"‚úÖ Ingested {len(texts)} chunks from {filepath.name}\")\n",
        "        return len(texts)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {filepath}: {e}\")\n",
        "        return 0\n",
        "\n",
        "def process_csv_files(folder_path, chunk_size=1000):\n",
        "    folder = Path(folder_path)\n",
        "    csv_files = list(folder.glob(\"**/*.csv\"))\n",
        "\n",
        "    total_chunks = 0\n",
        "    for filepath in csv_files:\n",
        "        chunks = ingest_csv_optimized(filepath, chunk_size=chunk_size)\n",
        "        total_chunks += chunks\n",
        "\n",
        "    print(f\"\\n‚úÖ Done: Ingested {total_chunks} chunks total from {len(csv_files)} CSV file(s).\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_csv_files(\"/content/drive/MyDrive/AI Chatbot Data\", chunk_size=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t5F3fpa4S6r",
        "outputId": "835a33e8-9c2d-4b51-c5cf-87473328db0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "üìÑ Processing: cleaned_glassdoor_reviews_large.new.csv (2012978 rows)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Chunks in cleaned_glassdoor_reviews_large.new.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2013/2013 [04:56<00:00,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ingested 2013 chunks from cleaned_glassdoor_reviews_large.new.csv\n",
            "\n",
            "‚úÖ Done: Ingested 2013 chunks total from 1 CSV file(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mp5SFf984Vod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ai_recruiter_with_rag.py (hardcoded base prompt)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib.util\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"/content\"):\n",
        "    try:\n",
        "        subprocess.check_call([\"apt-get\", \"install\", \"-y\", \"libmagic1\"])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to install apt dependency: {e}\")\n",
        "\n",
        "required = {\n",
        "    \"torch\": \"torch\",\n",
        "    \"pandas\": \"pandas\",\n",
        "    \"mammoth\": \"mammoth\",\n",
        "    \"docx\": \"python-docx\",\n",
        "    \"fitz\": \"PyMuPDF\",\n",
        "    \"xlrd\": \"xlrd\",\n",
        "    \"sentence_transformers\": \"sentence-transformers\",\n",
        "    \"transformers\": \"transformers --upgrade\",\n",
        "    \"gradio\": \"gradio\",\n",
        "    \"bitsandbytes\": \"git+https://github.com/TimDettmers/bitsandbytes.git\",\n",
        "    \"accelerate\": \"accelerate --upgrade\",\n",
        "    \"chromadb\": \"chromadb\",\n",
        "    \"magic\": \"python-magic\"\n",
        "}\n",
        "\n",
        "def install_missing(pkg_map):\n",
        "    for imp_name, pip_cmd in pkg_map.items():\n",
        "        if importlib.util.find_spec(imp_name) is None:\n",
        "            print(f\"üì¶ Installing {pip_cmd}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + pip_cmd.split())\n",
        "\n",
        "install_missing(required)\n",
        "\n",
        "import io\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "import mammoth\n",
        "import docx\n",
        "import fitz\n",
        "import xlrd\n",
        "import magic\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import gradio as gr\n",
        "from chromadb import PersistentClient\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "CHROMA_PATH = \"/content/drive/MyDrive/Glassdoor Chroma Store\"\n",
        "client = PersistentClient(path=CHROMA_PATH)\n",
        "collection = client.get_or_create_collection(\"csv_documents\")\n",
        "\n",
        "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "try:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"‚úÖ Loaded model with 4-bit quantization (bnb)\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Failed to load with bitsandbytes, falling back to fp16\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def extract_text_from_file(file):\n",
        "    if file is None:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        if hasattr(file, \"read\"):\n",
        "            bytes_data = file.read()\n",
        "        elif hasattr(file, \"value\") and os.path.exists(file.value):\n",
        "            with open(file.value, \"rb\") as f:\n",
        "                bytes_data = f.read()\n",
        "        elif hasattr(file, \"name\") and os.path.exists(file.name):\n",
        "            with open(file.name, \"rb\") as f:\n",
        "                bytes_data = f.read()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file object or missing file path.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to read uploaded file: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    mime_type = magic.from_buffer(bytes_data, mime=True)\n",
        "    stream = io.BytesIO(bytes_data)\n",
        "\n",
        "    try:\n",
        "        if mime_type == \"text/plain\":\n",
        "            return bytes_data.decode(\"utf-8\", errors=\"ignore\")[:3000]\n",
        "        elif mime_type == \"application/pdf\":\n",
        "            with fitz.open(stream=stream, filetype=\"pdf\") as doc:\n",
        "                return \"\\n\".join(page.get_text() for page in doc)[:3000]\n",
        "        elif mime_type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
        "            return \"\\n\".join(p.text for p in docx.Document(stream).paragraphs)[:3000]\n",
        "        elif mime_type == \"application/msword\":\n",
        "            return mammoth.extract_raw_text(stream).value[:3000]\n",
        "        elif mime_type == \"text/csv\":\n",
        "            return pd.read_csv(StringIO(bytes_data.decode(\"utf-8\", errors=\"ignore\"))).to_string()[:3000]\n",
        "        elif \"excel\" in mime_type:\n",
        "            return pd.read_excel(stream).to_string()[:3000]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Extraction error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def qa_with_llm(file, question):\n",
        "    base_prompt = \"\"\"You are a seasoned, trustworthy talent‚Äëacquisition specialist.\n",
        "Using insights from the Glassdoor‚ÄØJob‚ÄØReviews‚ÄØdataset, distill employee feedback to describe a company‚Äôs culture, work environment, benefits, challenges, and overall satisfaction.\n",
        "When asked, compare firms, spotlight strengths and weaknesses, and advise job‚Äëseekers on career moves.\n",
        "\n",
        "Your audience is actively exploring new opportunities and relies on you for clear, unbiased, actionable guidance in plain, professional language.\"\"\"\n",
        "\n",
        "    if file:\n",
        "        context = extract_text_from_file(file)\n",
        "    else:\n",
        "        query_embedding = embed_model.encode(question).tolist()\n",
        "        results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
        "        context = \"\\n\\n\".join(results['documents'][0]) if results and results['documents'] else \"\"\n",
        "\n",
        "    if not question.strip():\n",
        "        if not context:\n",
        "            return \"‚ö†Ô∏è Please upload a document or enter a question.\"\n",
        "        question = f\"What is the most relevant summary or insight based on the uploaded content?\"\n",
        "\n",
        "    prompt = f\"{base_prompt}\\n\\nQuestion: {question}\\n\\nContext:\\n{context[:1000]}\\n\\nAnswer:\"\n",
        "\n",
        "    tokenized = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096, padding=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in tokenized.items()}\n",
        "    print(f\"üß† Prompt token count: {inputs['input_ids'].shape[1]}\")\n",
        "\n",
        "    try:\n",
        "        start = time.time()\n",
        "        output = model.generate(**inputs, max_new_tokens=512, temperature=0.7, do_sample=True)\n",
        "        end = time.time()\n",
        "        print(f\"‚è±Ô∏è Response time: {end - start:.2f} sec\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Model generation failed: {e}\")\n",
        "        return \"‚ö†Ô∏è An error occurred while generating the answer. Please try with a different file or question.\"\n",
        "\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    final_answer = answer.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "app = gr.Interface(\n",
        "    fn=qa_with_llm,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload Document\"),\n",
        "        gr.Textbox(label=\"Ask a question\", placeholder=\"What are the top-rated companies for software engineers?\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"AI Recruiter Assistant\",\n",
        "    description=\"Upload a job-related document and/or ask the AI for recommendations or summaries.\"\n",
        ")\n",
        "\n",
        "app.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "1ADtCmjKnIRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.close()"
      ],
      "metadata": {
        "id": "kg85g3H1FHz7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
